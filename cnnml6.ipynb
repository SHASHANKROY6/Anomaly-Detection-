{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13025919,"sourceType":"datasetVersion","datasetId":8247543}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shashankroy568/cnnml6?scriptVersionId=262855121\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CNN Implementation on MVTec Dataset - FIXED VERSION\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport torchvision.transforms as transforms\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\nimport kagglehub\nfrom pathlib import Path\nfrom PIL import Image\nimport os\n\n# Configuration\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nBATCH_SIZE = 16\nEPOCHS = 50\nLEARNING_RATE = 0.001\n\ndef download_mvtec_dataset():\n    \"\"\"Download MVTec dataset\"\"\"\n    print(\"üì• Downloading MVTec dataset...\")\n    try:\n        dataset_path = kagglehub.dataset_download(\"shashankroy568/mvtec-anomaly-detection\")\n        print(f\"‚úÖ Dataset downloaded to: {dataset_path}\")\n        \n        root_path = Path(dataset_path)\n        \n        # Find MVTec categories\n        mvtec_categories = [\n            'bottle', 'cable', 'capsule', 'carpet', 'grid',\n            'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',\n            'tile', 'toothbrush', 'transistor', 'wood', 'zipper'\n        ]\n        \n        found_categories = {}\n        for root, dirs, files in os.walk(root_path):\n            for dir_name in dirs:\n                if dir_name in mvtec_categories:\n                    category_path = Path(root) / dir_name\n                    found_categories[dir_name] = category_path\n        \n        return root_path, found_categories\n    except Exception as e:\n        print(f\"‚ùå Dataset download error: {e}\")\n        return None, {}\n\nclass MVTecCNNDataset(Dataset):\n    \"\"\"MVTec dataset for CNN training\"\"\"\n    \n    def __init__(self, root_path, category, split='train', transform=None):\n        self.root_path = Path(root_path)\n        self.category = category\n        self.split = split\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.CenterCrop((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                              std=[0.229, 0.224, 0.225])\n        ])\n        \n        self.samples = self._load_samples()\n        print(f\"üìä CNN {category}: Loaded {len(self.samples)} {split} samples\")\n    \n    def _load_samples(self):\n        \"\"\"Load samples with train/test split for CNN\"\"\"\n        samples = []\n        \n        # Find category path\n        possible_paths = [\n            self.root_path / self.category,\n            self.root_path / \"mvtec_anomaly_detection\" / self.category,\n        ]\n        \n        category_path = None\n        for path in possible_paths:\n            if path.exists():\n                category_path = path\n                break\n        \n        if not category_path:\n            print(f\"‚ùå Category path not found for {self.category}\")\n            return samples\n        \n        # Collect all normal samples\n        train_good_path = category_path / 'train' / 'good'\n        test_good_path = category_path / 'test' / 'good'\n        \n        all_normal_samples = []\n        if train_good_path.exists():\n            for ext in ['*.png', '*.jpg', '*.jpeg', '*.bmp']:\n                for img_path in train_good_path.rglob(ext):\n                    all_normal_samples.append((str(img_path), 0))\n        \n        if test_good_path.exists():\n            for ext in ['*.png', '*.jpg', '*.jpeg', '*.bmp']:\n                for img_path in test_good_path.rglob(ext):\n                    all_normal_samples.append((str(img_path), 0))\n        \n        # Collect all anomaly samples\n        all_anomaly_samples = []\n        test_path = category_path / 'test'\n        if test_path.exists():\n            for defect_dir in test_path.iterdir():\n                if defect_dir.is_dir() and defect_dir.name != 'good':\n                    for ext in ['*.png', '*.jpg', '*.jpeg', '*.bmp']:\n                        for img_path in defect_dir.rglob(ext):\n                            all_anomaly_samples.append((str(img_path), 1))\n        \n        # Split data for CNN training (70% train, 30% test)\n        np.random.seed(42)\n        np.random.shuffle(all_normal_samples)\n        np.random.shuffle(all_anomaly_samples)\n        \n        normal_split = int(0.7 * len(all_normal_samples))\n        anomaly_split = int(0.7 * len(all_anomaly_samples))\n        \n        if self.split == 'train':\n            samples = all_normal_samples[:normal_split] + all_anomaly_samples[:anomaly_split]\n        else:  # test\n            samples = all_normal_samples[normal_split:] + all_anomaly_samples[anomaly_split:]\n        \n        # Count samples\n        normal_count = len([s for s in samples if s[1] == 0])\n        anomaly_count = len([s for s in samples if s[1] == 1])\n        \n        print(f\"   ‚úÖ {self.category} CNN {self.split}: {normal_count} normal, {anomaly_count} anomaly\")\n        \n        return samples\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        \n        try:\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, label\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n            dummy_image = torch.zeros((3, 224, 224))\n            return dummy_image, label\n\nclass CNN_Model(nn.Module):\n    \"\"\"CNN architecture for binary classification\"\"\"\n    \n    def __init__(self, num_classes=2):\n        super(CNN_Model, self).__init__()\n        \n        # Feature extraction layers\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.25),\n            \n            # Block 2\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.25),\n            \n            # Block 3\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.3),\n            \n            # Block 4\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d(0.3),\n        )\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((7, 7)),\n            nn.Flatten(),\n            nn.Linear(256 * 7 * 7, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\ndef calculate_metrics(y_true, y_pred_probs, threshold=0.5):\n    \"\"\"Calculate comprehensive metrics - FIXED VERSION\"\"\"\n    # Convert to numpy arrays if needed\n    if isinstance(y_true, list):\n        y_true = np.array(y_true)\n    if isinstance(y_pred_probs, list):\n        y_pred_probs = np.array(y_pred_probs)\n    \n    # Ensure proper data types\n    y_true = y_true.astype(int)\n    y_pred_probs = y_pred_probs.astype(float)\n    \n    # Create binary predictions\n    y_pred = (y_pred_probs >= threshold).astype(int)\n    \n    # Calculate metrics\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    auc = roc_auc_score(y_true, y_pred_probs) if len(set(y_true)) > 1 else 0.5\n    \n    # Confusion matrix\n    try:\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    except ValueError:\n        # Handle case where confusion matrix can't be calculated\n        tp = fp = tn = fn = 0\n    \n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n    \n    return {\n        'auc': auc,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'specificity': specificity,\n        'accuracy': accuracy,\n        'true_positives': int(tp),\n        'true_negatives': int(tn),\n        'false_positives': int(fp),\n        'false_negatives': int(fn)\n    }\n\ndef train_cnn_single_category(root_path, category, results_dict):\n    \"\"\"Train CNN on a single category\"\"\"\n    \n    print(f\"\\n\" + \"=\"*50)\n    print(f\"üß† TRAINING CNN: {category.upper()}\")\n    print(f\"=\"*50)\n    \n    start_time = time.time()\n    \n    try:\n        # Create datasets\n        train_dataset = MVTecCNNDataset(root_path, category, 'train')\n        test_dataset = MVTecCNNDataset(root_path, category, 'test')\n        \n        if len(train_dataset) == 0 or len(test_dataset) == 0:\n            print(f\"‚ö†Ô∏è Insufficient data for {category}\")\n            results_dict[category] = {'status': 'failed', 'reason': 'insufficient_data'}\n            return\n        \n        # Check class distribution\n        train_labels = [train_dataset.samples[i][1] for i in range(len(train_dataset))]\n        test_labels = [test_dataset.samples[i][1] for i in range(len(test_dataset))]\n        \n        if len(set(train_labels)) < 2 or len(set(test_labels)) < 2:\n            print(f\"‚ö†Ô∏è {category}: Single class detected\")\n            results_dict[category] = {'status': 'failed', 'reason': 'single_class'}\n            return\n        \n        # Handle class imbalance with weighted sampling\n        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n        sample_weights = [class_weights[label] for label in train_labels]\n        sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n        \n        # Create data loaders\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\n        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n        \n        print(f\"‚úÖ {category}: Dataset ready - {len(train_dataset)} train, {len(test_dataset)} test\")\n        \n        # Initialize model\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = CNN_Model(num_classes=2).to(device)\n        \n        # Loss function and optimizer with class weights\n        class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n        \n        print(f\"üîÑ Training CNN for {category}...\")\n        \n        # Training loop\n        best_auc = 0\n        patience_counter = 0\n        patience = 10\n        best_predictions = []\n        best_targets = []\n        \n        for epoch in range(EPOCHS):\n            # Training\n            model.train()\n            train_loss = 0\n            train_correct = 0\n            train_total = 0\n            \n            for batch_idx, (data, target) in enumerate(train_loader):\n                data, target = data.to(device), target.to(device)\n                \n                optimizer.zero_grad()\n                output = model(data)\n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n                \n                train_loss += loss.item()\n                _, predicted = torch.max(output.data, 1)\n                train_total += target.size(0)\n                train_correct += (predicted == target).sum().item()\n            \n            train_acc = 100. * train_correct / train_total\n            \n            # Validation\n            model.eval()\n            val_predictions = []\n            val_targets = []\n            \n            with torch.no_grad():\n                for data, target in test_loader:\n                    data, target = data.to(device), target.to(device)\n                    output = model(data)\n                    prob = torch.softmax(output, dim=1)[:, 1]  # Probability of anomaly class\n                    \n                    val_predictions.extend(prob.cpu().numpy())\n                    val_targets.extend(target.cpu().numpy())\n            \n            # Calculate AUC\n            val_auc = roc_auc_score(val_targets, val_predictions) if len(set(val_targets)) > 1 else 0.5\n            scheduler.step(1 - val_auc)  # Use AUC for learning rate scheduling\n            \n            # Early stopping based on AUC\n            if val_auc > best_auc:\n                best_auc = val_auc\n                patience_counter = 0\n                # Save best model predictions\n                best_predictions = val_predictions.copy()\n                best_targets = val_targets.copy()\n            else:\n                patience_counter += 1\n            \n            if epoch % 10 == 0:\n                print(f\"   Epoch {epoch}: Train Acc = {train_acc:.2f}%, Val AUC = {val_auc:.4f}\")\n            \n            if patience_counter >= patience:\n                print(f\"   Training completed at epoch {epoch}\")\n                break\n        \n        # Calculate final metrics using best model - FIXED VERSION\n        if len(best_predictions) > 0 and len(best_targets) > 0:\n            metrics = calculate_metrics(best_targets, best_predictions)\n        else:\n            # Fallback metrics if no best predictions saved\n            metrics = {\n                'auc': 0.5,\n                'precision': 0.0,\n                'recall': 0.0,\n                'f1_score': 0.0,\n                'specificity': 0.0,\n                'accuracy': 0.0,\n                'true_positives': 0,\n                'true_negatives': 0,\n                'false_positives': 0,\n                'false_negatives': 0\n            }\n        \n        training_time = time.time() - start_time\n        \n        # Store results\n        results_dict[category] = {\n            'status': 'success',\n            'auc_score': metrics['auc'],\n            'precision': metrics['precision'],\n            'recall': metrics['recall'],\n            'f1_score': metrics['f1_score'],\n            'specificity': metrics['specificity'],\n            'accuracy': metrics['accuracy'],\n            'true_positives': metrics['true_positives'],\n            'true_negatives': metrics['true_negatives'],\n            'false_positives': metrics['false_positives'],\n            'false_negatives': metrics['false_negatives'],\n            'train_samples': len(train_dataset),\n            'test_samples': len(test_dataset),\n            'training_time': training_time,\n            'model_type': 'cnn',\n            'epochs_trained': epoch + 1\n        }\n        \n        print(f\"‚úÖ {category}: AUC Score = {metrics['auc']:.4f}\")\n        print(f\"üìä {category}: Precision = {metrics['precision']:.4f}, Recall = {metrics['recall']:.4f}, F1 = {metrics['f1_score']:.4f}\")\n        print(f\"‚è±Ô∏è  {category}: Training time = {training_time:.1f}s\")\n        \n    except Exception as e:\n        print(f\"‚ùå {category}: Training failed - {e}\")\n        import traceback\n        traceback.print_exc()\n        results_dict[category] = {'status': 'failed', 'reason': str(e)}\n\ndef run_cnn_pipeline():\n    \"\"\"Run CNN training pipeline\"\"\"\n    \n    print(\"üß† Starting CNN Training Pipeline\")\n    print(\"=\" * 50)\n    \n    # Download dataset\n    root_path, categories = download_mvtec_dataset()\n    \n    if not root_path or not categories:\n        print(\"‚ùå Cannot proceed without dataset\")\n        return\n    \n    # Target categories\n    target_categories = ['bottle', 'metal_nut', 'capsule', 'cable', 'screw', 'pill', 'transistor', 'hazelnut']\n    available_targets = [cat for cat in target_categories if cat in categories]\n    \n    if not available_targets:\n        print(\"‚ùå No target categories found\")\n        return\n    \n    print(f\"\\nüéØ Will train CNN on {len(available_targets)} categories: {available_targets}\")\n    \n    # Train each category\n    cnn_results = {}\n    total_start_time = time.time()\n    \n    for i, category in enumerate(available_targets, 1):\n        print(f\"\\nüîÑ Progress: {i}/{len(available_targets)} categories\")\n        train_cnn_single_category(root_path, category, cnn_results)\n    \n    total_time = time.time() - total_start_time\n    \n    # Results summary\n    print(f\"\\n\" + \"=\"*50)\n    print(\"üìã CNN TRAINING RESULTS\")\n    print(\"=\"*50)\n    \n    successful_trainings = [cat for cat, res in cnn_results.items() if res.get('status') == 'success']\n    failed_trainings = [cat for cat, res in cnn_results.items() if res.get('status') == 'failed']\n    \n    print(f\"‚úÖ Successful: {len(successful_trainings)}/{len(available_targets)} categories\")\n    print(f\"‚ùå Failed: {len(failed_trainings)}/{len(available_targets)} categories\")\n    print(f\"‚è±Ô∏è  Total time: {total_time:.1f}s\")\n    \n    if successful_trainings:\n        print(f\"\\nüìä DETAILED CNN RESULTS:\")\n        print(\"-\" * 120)\n        print(f\"{'Category':<12} {'AUC':<8} {'Precision':<9} {'Recall':<8} {'F1':<8} {'Acc':<8} {'Train':<6} {'Test':<5} {'Time':<6}\")\n        print(\"-\" * 120)\n        \n        for category in successful_trainings:\n            res = cnn_results[category]\n            print(f\"{category:<12} {res['auc_score']:<8.4f} {res['precision']:<9.4f} {res['recall']:<8.4f} \"\n                  f\"{res['f1_score']:<8.4f} {res['accuracy']:<8.4f} {res['train_samples']:<6} {res['test_samples']:<5} \"\n                  f\"{res['training_time']:<6.1f}s\")\n        \n        # Calculate averages\n        avg_auc = np.mean([cnn_results[cat]['auc_score'] for cat in successful_trainings])\n        avg_precision = np.mean([cnn_results[cat]['precision'] for cat in successful_trainings])\n        avg_recall = np.mean([cnn_results[cat]['recall'] for cat in successful_trainings])\n        avg_f1 = np.mean([cnn_results[cat]['f1_score'] for cat in successful_trainings])\n        avg_accuracy = np.mean([cnn_results[cat]['accuracy'] for cat in successful_trainings])\n        \n        print(\"-\" * 120)\n        print(f\"{'AVERAGE':<12} {avg_auc:<8.4f} {avg_precision:<9.4f} {avg_recall:<8.4f} \"\n              f\"{avg_f1:<8.4f} {avg_accuracy:<8.4f}\")\n        print(\"-\" * 120)\n    \n    # Export results\n    if successful_trainings:\n        cnn_df = pd.DataFrame([\n            {\n                'category': category,\n                'model': 'CNN',\n                'auc_score': cnn_results[category]['auc_score'],\n                'precision': cnn_results[category]['precision'],\n                'recall': cnn_results[category]['recall'],\n                'f1_score': cnn_results[category]['f1_score'],\n                'accuracy': cnn_results[category]['accuracy'],\n                'train_samples': cnn_results[category]['train_samples'],\n                'test_samples': cnn_results[category]['test_samples'],\n                'training_time': cnn_results[category]['training_time'],\n                'model_type': cnn_results[category]['model_type'],\n                'epochs_trained': cnn_results[category]['epochs_trained']\n            }\n            for category in successful_trainings\n        ])\n        \n        cnn_df.to_csv('mvtec_cnn_results.csv', index=False)\n        print(f\"\\nüíæ CNN results exported to: mvtec_cnn_results.csv\")\n    \n    print(f\"\\n‚úÖ CNN training pipeline completed!\")\n    \n    return cnn_results\n\n# Run the CNN pipeline\nif __name__ == \"__main__\":\n    cnn_results = run_cnn_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:32:03.86108Z","iopub.execute_input":"2025-09-13T08:32:03.861406Z","iopub.status.idle":"2025-09-13T08:32:06.860156Z","shell.execute_reply.started":"2025-09-13T08:32:03.861352Z","shell.execute_reply":"2025-09-13T08:32:06.858914Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2949024805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CNN Implementation on MVTec Dataset - FIXED VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeightedRandomSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}